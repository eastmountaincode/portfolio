{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "caselawAPIFYforPortfolio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMIh2ncjrTD90+5xg8BvaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eastmountaincode/portfolio/blob/main/caselawAPIFYforPortfolio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Yx6cK0bjVf"
      },
      "source": [
        "## Pulling data from the Caselaw Access Project API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMi3bupcbMQI"
      },
      "source": [
        "From https://case.law/about/:\n",
        "\n",
        "\"*The **Caselaw Access Project** (“CAP”) expands public access to U.S. law. Our goal is to make all published U.S. court decisions freely available to the public online, in a consistent format, digitized from the collection of the Harvard Law School Library.*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDIL7rhfxPOT"
      },
      "source": [
        "This task was connected to the University of Cincinnati's Digital Scholarship's project investigating the Anthropocene. \n",
        "https://www.cambridge.org/core/journals/pmla/article/abs/anthropocene-and-empire-discourse-networks-of-the-human-record/367FA0820A5AB8F59755010E37DC7B48"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBJ98yG2xrs3"
      },
      "source": [
        "It was my job to pull all caselaw documents containing the word \"bomb\", \"atom\", \"nuclear\", \"pollution\", \"climate\", \"environment\", \"earth\", or \"enivronmental\". This code writes the caselaw document data directly to a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpJw7cEHbeDq"
      },
      "source": [
        "## The code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkutQbs7ZeXE"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import csv\n",
        "\n",
        "id_df = pd.DataFrame(columns = ['ID'])\n",
        "\n",
        "with open('str8_2_hell.csv', mode='w') as csv_file:\n",
        "    fieldnames = ['ID', 'URL', 'date', 'title', 'text']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    \n",
        "termlist = [\"bomb\", \"atom\", \"nuclear\", \"pollution\", \"climate\", \"environment\", \"earth\", \"environmental\"]\n",
        "\n",
        "#for each term...\n",
        "\n",
        "for termi in termlist:\n",
        "\n",
        "  #get the first one, which will serve as a foundation for getting the others\n",
        "\n",
        "  firsturl = 'https://api.case.law/v1/cases/?full_case=true&search={term}'.format(term = termi)\n",
        "\n",
        "  res = requests.get(\n",
        "      firsturl,\n",
        "      headers={'Authorization': 'Token <API KEY GOES HERE>'}\n",
        "  )\n",
        "  res = res.json()\n",
        "\n",
        "  #get total doc count, which will be useful for determining how many times to iterate\n",
        "\n",
        "  docCount = res['count']\n",
        "  counter = 0\n",
        "  alt_case_counter = 0\n",
        "\n",
        "  nextURL = res['next']\n",
        "\n",
        "  #big ol while loop\n",
        "\n",
        "  while nextURL:\n",
        "    for i in range(len(res['results'])):\n",
        "      #get the id\n",
        "      docID = str(res['results'][i]['id'])\n",
        "\n",
        "      #make sure there actually is a casebody we can pull\n",
        "      if res['results'][i]['casebody']['data']['opinions']:\n",
        "\n",
        "        #check if the docID is already in the dataframe \n",
        "        if not (id_df['ID']==docID).any():\n",
        "          #if not, get the info and add it to the df\n",
        "          #we already have the id...\n",
        "          #get the URL\n",
        "          URL = res['results'][i]['url']\n",
        "          #get the date\n",
        "          date = res['results'][i]['decision_date']\n",
        "          #get the title\n",
        "          title = res['results'][i]['name_abbreviation']\n",
        "          #get the full text!! o.0\n",
        "          text = res['results'][i]['casebody']['data']['opinions'][0]['text']\n",
        "          text = text.replace(',', '')\n",
        "          #add the info to a new row in the dataframe\n",
        "          new_row = {'ID': docID, 'URL':URL, 'date': date, \"title\":title, \"text\":text}\n",
        "          new_dfrow = {'ID': docID}\n",
        "          id_df = id_df.append(new_dfrow, ignore_index=True)\n",
        "\n",
        "          with open('str8_2_hell.csv', mode='a') as csv_file:\n",
        "              fieldnames = ['ID', 'URL', 'date', 'title', 'text']\n",
        "              writer = csv.DictWriter(csv_file, fieldnames=fieldnames)         \n",
        "              writer.writerow(new_row)\n",
        "      else:\n",
        "        alt_case_counter += 1\n",
        "         #check if the docID is already in the dataframe \n",
        "        if not (id_df['ID']==docID).any():\n",
        "          #if not, get the info and add it to the df\n",
        "          #we already have the id...\n",
        "          #get the URL\n",
        "          URL = res['results'][i]['url']\n",
        "          #get the date\n",
        "          date = res['results'][i]['decision_date']\n",
        "          #get the title\n",
        "          title = res['results'][i]['name_abbreviation']\n",
        "          #get the full text!! o.0\n",
        "          text = res['results'][i]['casebody']['data']['head_matter']\n",
        "          text = text.replace(',', '')\n",
        "          #add the info to a new row in the dataframe\n",
        "          new_row = {'ID': docID, 'URL':URL, 'date': date, \"title\":title, \"text\":text}\n",
        "          new_dfrow = {'ID': docID}\n",
        "          id_df = id_df.append(new_dfrow, ignore_index=True)\n",
        "\n",
        "          with open('str8_2_hell.csv', mode='a') as csv_file:\n",
        "              fieldnames = ['ID', 'URL', 'date', 'title', 'text']\n",
        "              writer = csv.DictWriter(csv_file, fieldnames=fieldnames)   \n",
        "              writer.writerow(new_row)\n",
        "\n",
        "    counter += len(res['results'])  \n",
        "    percent_done = round(((counter / docCount) * 100), 2)\n",
        "    print(\"{percent_done} percent done\".format(percent_done = percent_done))\n",
        "      \n",
        "    #get the next url\n",
        "    nextURL = res['next']\n",
        "    if bool(nextURL) == True:\n",
        "      #get the next doc json\n",
        "      res = requests.get(\n",
        "        nextURL,\n",
        "        headers={'Authorization': 'Token <API KEY GOES HERE>'}\n",
        "      )\n",
        "\n",
        "      res = res.json()\n",
        "    else:\n",
        "      break\n",
        "  print(termi)\n",
        "\n",
        "print(\"Completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMDI7EE1av4e",
        "outputId": "229d6026-1991-4045-cd35-f8e95a2e4137"
      },
      "source": [
        "print(\"We have\", alt_case_counter, \"alt cases.\")\n",
        "print(id_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 0 alt cases.\n",
            "           ID\n",
            "0     5311147\n",
            "1    10910713\n",
            "2       82210\n",
            "3     3819892\n",
            "4     3958351\n",
            "..        ...\n",
            "395  11319933\n",
            "396    175196\n",
            "397   1306459\n",
            "398   4131365\n",
            "399   2657162\n",
            "\n",
            "[400 rows x 1 columns]\n"
          ]
        }
      ]
    }
  ]
}